Mitigating Bias in ML Models using MinDiff
This project demonstrates how to address fairness concerns in machine learning models using the MinDiff remediation technique. The goal is to ensure that a text classifier performs equitably across different sensitive demographic groups.
Key Objectives
Fairness Analysis: Evaluated model performance using "equal opportunity" fairness metrics.
Bias Mitigation: Applied the MinDiff technique to a TensorFlow model to reduce performance gaps between sensitive and non-sensitive datasets.
Model Evaluation: Improved False Positive Rates (FPR) for underperforming groups to meet production standards.
Tools & Technologies
TensorFlow & Keras: For model building and remediation.
Google Cloud Vertex AI: Used Workbench instances for scalable model training and evaluation.
Python: Core programming language for implementation.
Results:
Successfully reduced the disparity in model predictions, ensuring a more responsible and fair AI deployment.
